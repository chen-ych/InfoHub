{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-Learning  \n",
    "During class, we have learnt reinforcement learning. In this assignment, you need to design a Deep Q-Learning network to play the *CartPole* game. Our experiment consists of three parts:\n",
    "\n",
    "1. Introduction to CartPole game\n",
    "2. Implementation of Deep Q-Learning network\n",
    "3. Analytical questions.\n",
    "\n",
    "First, let us import some necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from IPython import display\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to CartPole\n",
    "[CartPole](https://gym.openai.com/envs/CartPole-v1/) is a classical environment from [gym](https://gym.openai.com/) package that is designed by [OpenAI](https://openai.com/). In CartPole environment, a pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pendulum will fall over due to gravity if the cart doesn't move.\n",
    "\n",
    "![](./fig.jpg)\n",
    "\n",
    "Our goal is to balance the pole on the cart. We have:\n",
    "- Continuous state space: $(x, \\theta, \\dot{x}, \\dot{\\theta})$ \n",
    "- Discrete action space: force $F\\in\\{0, 1\\}$, $0$ for left force, $1$ for right force\n",
    "- Reward: $+5$ if the pole is upright, $-5$ if the pole is more than $15$ degrees from vertical, or the cart moves more than $2.4$ units from the center.\n",
    "\n",
    "To run CartPole, you need to install the `gym` package:\n",
    "```py\n",
    "pip install gym\n",
    "pip install pyglet\n",
    "```\n",
    "Then import `gym` packages into our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in d:\\programsupport\\anaconda\\lib\\site-packages (0.21.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in d:\\programsupport\\anaconda\\lib\\site-packages (from gym) (1.20.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in d:\\programsupport\\anaconda\\lib\\site-packages (from gym) (1.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install gym\n",
    "!pip install pyglet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of Deep Q-Learning\n",
    "\n",
    "Give state space $S$, action space $A$, and policy $\\pi$, the Q-value function $Q^\\pi(s, a)$ is defined as the expected payoff if we take action $a\\in A$ at state $s \\in S$ and follow $\\pi$:\n",
    "$$\n",
    "Q^{\\pi}(s, a)=\\mathbb{E}\\left[\\sum_{t \\geq 0} \\gamma^{t} r_{t} \\mid s_{0}=s, a_{0}=a, \\pi\\right],\n",
    "$$\n",
    "where $\\gamma$ is the discount factor, $r_t$ is the reward at time step $t$. So the the optimal Q-value function is \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Q^{*}(s, a)&=\\max _{\\pi} Q^{\\pi}(s, a)=\\max _{\\pi} \\mathbb{E}\\left[\\sum_{t \\geq 0} \\gamma^{t} r_{t} \\mid s_{0}=s, a_{0}=a, \\pi\\right]\\\\&=\\mathbb{E}_{s^{\\prime} \\sim \\mathcal{E}}\\left[r+\\gamma \\max _{a^{\\prime}} Q^{*}\\left(s^{\\prime}, a^{\\prime}\\right) \\mid s_{0}=s, a_{0}=a\\right].\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "In deep Q-learning, we use a neural network $Q_{\\theta}$ to approximate $Q^{*}(s, a)$. Basically, the process can be described in following way:\n",
    "\n",
    "When we apply action $a_t$ at state $s_t$, the environment gives a feedback that consists of next state $s_{t+1}$ and reward $r_t$. So we get a sample $(s_t, a_t, r_t, s_{t+1})$. For the next step, we can get another sample $(s_{t+1}, a_{t+1}, r_{t+1}, s_{t+2})$ similarly. Apparently, these samples are highly related. Using these samples to train our model invalidates the i.i.d. assumption on training samples. Therefore, experience replay was proposed to reduce sample correlation by using random mini-batch sampled from a replay memory $\\mathcal D$. The exact algorithm is as follows:\n",
    "\n",
    ">Initialize replay memory $\\mathcal D$ to capacity $N$<br>\n",
    "Initialize the Q-value model $Q_\\theta$ with random weights<br> \n",
    "For episode $=1, M$ do \\{<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Initialize state $s_1$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;For $t=1, T$ do \\{<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;With probability $\\epsilon$ select a random action $a_t$,<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;otherwise select $a_t = \\arg\\max_{a}Q_\\theta\\left(s_{t}, a \\right)$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Apply action $a_t$ and get reward $r_t$ and next state $s_{t+1}$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Store transition $(s_t, a_t, r_t, s_{t+1})$ in memory $\\mathcal D$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Sample random minibatch of transitions $\\Omega$ from $\\mathcal D$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;For each sample $(s_j, a_j, r_j, s_{j+1})$ in minibatch $\\Omega$\\{<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Set $y_{j}=\\left\\{\\begin{array}{ll}\n",
    "r_{j} &s_{j+1}\\text{ is terminal state}\\\\\n",
    "r_{j}+\\gamma \\max _{a} Q_{\\theta}\\left(s_{j+1}, a\\right)&s_{j+1}\\text{ is not terminal state}\n",
    "\\end{array}\\right.$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Perform a gradient descent step using $\\left(y_{j}-Q_{\\theta}\\left(s_{j}, a_{j}\\right)\\right)^{2}$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\\}<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\\}<br>\n",
    "\\}<br>\n",
    "\n",
    "Here, $\\epsilon$ is the exploration rate.\n",
    "\n",
    "We will use a *deque* object as memory $\\mathcal D$ to store transitions. Also, we tried to build a simple neural network (`SimpleNN`) to fit an arbitrary function in PA3. We are going to re-use `SimpleNN` to build a **q-model** for deep Q-learning. Here, we provide you a python file named `nn.py` so you don't need to copy your codes. Now, let us import `nn` and `deque` so we can use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from nn import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can set other parameters and construct a *deque* object as our memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global parameters\n",
    "gamma = 0.95    # discount rate\n",
    "epsilon = 1.0  # exploration rate\n",
    "epsilon_min = 0.01 # lower bound of exploration\n",
    "epsilon_decay = 0.995 # epsilon decay rate\n",
    "N = 2000 # capacity of memory\n",
    "M = 1000 # maximum of episode\n",
    "T = 500 # maximum of time step t\n",
    "memory = deque(maxlen=N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because there are only two possible actions ($0$ or $1$), we can directly predict the Q-values of applying two actions at a state instead of using action as a part of input features:\n",
    "$$\n",
    "Q_\\theta(s) \\approx \\begin{bmatrix}Q^*(s, 0) \\\\ Q^*(s, 1) \\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "To achieve it, we just need to set the output dimension to $2$. The q-model takes states as input, so the input dimension is set to $4$.  Other parts can remain the same as PA3, so our q-model contains:\n",
    "- L0, Input layer (shape: $N \\times 4$), where $N$ is the batch size.\n",
    "- L1, Linear layer (shape: $4 \\times 80$)\n",
    "- L2, ReLU layer\n",
    "- L3, Linear layer (shape: $80 \\times 80$)\n",
    "- L4, ReLU layer\n",
    "- L5, Output layer (linear layer, shape: $80 \\times 2$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cartpole game\n",
    "env = gym.make('CartPole-v1')\n",
    "#### info related to state and actions\n",
    "# the length of state vector, and state is continuous\n",
    "state_vec_len = env.observation_space.shape[0]\n",
    "print(state_vec_len)\n",
    "# the size of action set\n",
    "action_size = env.action_space.n\n",
    "layers = [ \n",
    "    # input layer is input data\n",
    "    # L1\n",
    "    Linear(state_vec_len, 80),\n",
    "    # L2\n",
    "    ReLU(),\n",
    "    # L3\n",
    "    Linear(80, 80),\n",
    "    # L4\n",
    "    ReLU(),\n",
    "    # L5\n",
    "    Linear(80, action_size)\n",
    "]\n",
    "loss = MSELoss()\n",
    "# sgd: lr = 0.0005\n",
    "# bgd: lr = 0.001\n",
    "q_model = SimpleNN(layers, loss, lr=0.001)   # generate nets and DQNagent model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each time step, we need to apply an action at current state. Here, we define a function `act` to select the action. The `act` function takes current state as input and returns the selected action. \n",
    "\n",
    "**Q1. Please complete `act` with your implementation to select an action at given state. The corresponding pseudocode is listed as follows.**\n",
    "\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;With probability $\\epsilon$ select a random action $a_t$,<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;otherwise select $a_t = \\arg\\max_{a}Q_\\theta\\left(s_{t}, a \\right)$<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select an action\n",
    "def act(state):\n",
    "    ### start of your codes\n",
    "    statenp = np.array(state)\n",
    "    return np.argmax(q_model(statenp))\n",
    "    ### end of your codes\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After applying an action, the environment will give us the next state and reward. In our situation, it also tell us if the state is terminal. We use `remember` functions to store current transition in memory $\\mathcal D$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remember(memory, state, action, reward, next_state, terminal):\n",
    "    memory.append((state, action, reward, next_state, terminal))\n",
    "    return memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can randomly sample a minibatch from $\\mathcal D$ and train our q-model using it. This process is called *experience replay*, and we define a `replay` function to do this. The input parameter of `replay` is memory $\\mathcal D$, our q-model, and batch_size. After training, it returns the updated q-model.   \n",
    "\n",
    "**Q2. Please complete `replay` to train our q-model. The corresponding pseudocode is listed as follows. *Hint: If you are using SGD, the recommended learning rate is 0.0005; if you are using BGD, the recommended learning rate is 0.001.***\n",
    "\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Sample random minibatch of transitions $\\Omega$ from $\\mathcal D$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;For each sample $(s_j, a_j, r_j, s_{j+1})$ in minibatch $\\Omega$\\{<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Set $y_{j}=\\left\\{\\begin{array}{ll}\n",
    "r_{j} &s_{j+1}\\text{ is terminal state}\\\\\n",
    "r_{j}+\\gamma \\max _{a} Q_{\\theta}\\left(s_{j+1}, a\\right)&s_{j+1}\\text{ is not terminal state}\n",
    "\\end{array}\\right.$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Perform a gradient descent step using $\\left(y_{j}-Q_{\\theta}\\left(s_{j}, a_{j}\\right)\\right)^{2}$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\\}<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replay(memory, q_model, batch_size):\n",
    "    minibatch = random.sample(memory, batch_size)\n",
    "    states = np.array([minibatch[i][0] for i in range(batch_size)]) \n",
    "    actions = np.array([minibatch[i][1] for i in range(batch_size)]).astype(int)\n",
    "    rewards = np.array([minibatch[i][2] for i in range(batch_size)]) \n",
    "    next_states = np.array([minibatch[i][3] for i in range(batch_size)]) \n",
    "    terminal = np.array([minibatch[i][4] for i in range(batch_size)]).astype(float)\n",
    "    gamma1 = .001\n",
    "    #print(next_states[1])\n",
    "    #print(\"qmodel(nst)=\",q_model(next_states).shape,\"amaxqmd=\",np.amax(q_model(next_states),axis=1).shape)\n",
    "    ### start of your codes\n",
    "    Y = rewards + (1-terminal)*gamma1*np.amax(q_model(next_states),axis=1)\n",
    "    predy = q_model(states)\n",
    "    #print(\"predy=\",predy.shape)\n",
    "    onehot_action = np.eye(2)[actions]\n",
    "    #print(onehot_action)\n",
    "    onehot_y = np.array([ [Y[i] * onehot_action[i][0],Y[i]*onehot_action[i][1]] for i in range(batch_size)])\n",
    "    #print(\"o11ny=\",onehot_y.shape)\n",
    "    onehot_y = np.where(onehot_action==1.,onehot_y,predy)\n",
    "    #print(\"ony=\",onehot_y.shape)\n",
    "    print(onehot_y-predy)\n",
    "    q_model.backward(onehot_y)\n",
    "    ### end of your codes\n",
    "    return q_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have achieved most parts of our deep Q-learning network. In the following code, we set the game environment to get feedback based on current state and action. The aboving functions you implemented will be used to get a higher score, which is just the number of time steps the agent passed. We will compute the mean score after every certain number of episodes and visualize the results. A baseline of random action selection is maintained so you can see the performance clearly. Also, the log of training process is maintained in `log.txt` where you can find the scores of all episodes. After each episode, the exploration rate decays.\n",
    "\n",
    "Now, run following codes and good luck! Please attach the figure and log into your submission. *Hint: after about 200 episodes, your score should oscillate around 150. It is normal to occasionally get some extremely low scores in the later stages of training. If you cannont get a reasonable result, your assignment will be graded based on your implementation.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### for visualization\n",
    "# for deep q-learning\n",
    "mean_score_dqn = []\n",
    "score_dqn = 0\n",
    "# for random action\n",
    "mean_score_random = []\n",
    "score_random = 0\n",
    "# visualize the mean score of each plot_n steps\n",
    "plot_n = 10\n",
    "batch_size = 100\n",
    "plt.figure(figsize=(10, 7))\n",
    "for episode in range(M):\n",
    "    state_dqn = env.reset() \n",
    "    for time in range(T):\n",
    "        #env.render()      # show the amination\n",
    "        #print(state_dqn)\n",
    "        action = act(state_dqn)     # chose action\n",
    "        next_state, reward, terminal, _ = env.step(action) # get feedback\n",
    "        reward = 5 if not terminal else -5  # get -10 reward if fail\n",
    "        memory = remember(memory, state_dqn, action, reward, next_state, terminal) # store transition\n",
    "        state_dqn = next_state\n",
    "        if len(memory) > batch_size and time % 5 == 0: # train q_model\n",
    "            q_model = replay(memory, q_model, batch_size)\n",
    "        if terminal:                \n",
    "            print(\"episode: {}/{}, score: {}, epsilon: {:.2}\"\n",
    "                  .format(episode, M, time, epsilon), file=open('./log1.txt', 'a'))\n",
    "            break\n",
    "    score_dqn += time\n",
    "    # random action\n",
    "    state_random = env.reset()\n",
    "    for time in range(500):\n",
    "        action = random.randint(0, action_size-1)\n",
    "        next_state, _, terminal, _ = env.step(action) # get feedback\n",
    "        state_random = next_state\n",
    "        if terminal:                \n",
    "            break\n",
    "    score_random += time\n",
    "    if (episode+1) % plot_n == 0:\n",
    "        display.clear_output()\n",
    "        plt.clf()\n",
    "        mean_score_dqn.append(score_dqn / plot_n)\n",
    "        mean_score_random.append(score_random / plot_n)\n",
    "        score_dqn = 0\n",
    "        score_random = 0\n",
    "        plt.title('expisode={}'.format(episode+1))\n",
    "        plt.plot(mean_score_dqn, label=\"DQN\")\n",
    "        plt.plot(mean_score_random, label=\"random\")\n",
    "        plt.legend()\n",
    "        display.display(plt.gcf())\n",
    "    if epsilon > epsilon_min:\n",
    "        epsilon *= epsilon_decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analytical questions\n",
    "**Q3. Explain the role of exploration rate.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
